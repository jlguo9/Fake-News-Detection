{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following news article real or fake? Please provide a brief explanation as well.\n",
      "\n",
      "News article:\n",
      "Scientists have discovered a new species of spider in the Amazon rainforest. The spider is unique because it can fly using wings that extend from its abdomen. Researchers believe this adaptation allows the spider to escape predators and search for food more efficiently.\n",
      "\n",
      "Answer:????\n",
      "\n",
      "The spider is a member of the genus Apis mellifera. It is a member of the genus Ap\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def is_fake_news_gpt2(news_text):\n",
    "    prompt = f\"Is the following news article real or fake? Please provide a brief explanation as well.\\n\\nNews article:\\n{news_text}\\n\\nAnswer: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    answer = answer.replace(prompt, \"\").strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "news_text = \"\"\"Scientists have discovered a new species of spider in the Amazon rainforest. The spider is unique because it can fly using wings that extend from its abdomen. Researchers believe this adaptation allows the spider to escape predators and search for food more efficiently.\"\"\"\n",
    "\n",
    "result = is_fake_news_gpt2(news_text)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [''.join(c for c in token if c not in string.punctuation) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming/Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_data(\"C:\\\\Users\\\\Alex\\\\Downloads\\\\WELFake_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create document vectors using the mean of word vectors\n",
    "def document_vector(tokens, model):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "data['document_vector'] = data['tokens'].apply(lambda x: document_vector(x, word2vec_model))\n",
    "\n",
    "# Extract features (document vectors) and labels\n",
    "X = np.stack(data['document_vector'].values)\n",
    "y = data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48924097  0.12713179 -0.27337244 ... -0.26956478 -0.13909675\n",
      "   1.03828049]\n",
      " [ 2.66600227  0.05235812 -0.88148677 ... -0.13322905 -0.52463275\n",
      "   0.99650037]\n",
      " [ 0.70893848 -0.34301457  0.21790981 ... -0.39881271  0.02146312\n",
      "  -0.21664295]\n",
      " ...\n",
      " [-0.27936357 -0.64543819  0.28852254 ... -0.36916664 -0.30971491\n",
      "   0.52842015]\n",
      " [ 0.05290093 -0.29853812 -0.04654996 ... -0.12153742  0.16454835\n",
      "   0.71198934]\n",
      " [ 1.42836714 -0.61658293  0.15999073 ... -0.16453208  0.36877975\n",
      "   0.72680187]]\n",
      "[1 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X and y to .npy files\n",
    "np.save('X.npy', X)\n",
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and y from .npy files\n",
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Define a simple RNN model (use with padded sequences)\n",
    "def create_rnn_model(input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=input_length))\n",
    "    model.add(SimpleRNN(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models[\"RNN\"] = create_rnn_model(input_length=your_padded_sequence_length)\n",
    "\n",
    "# Define a simple LSTM model (use with padded sequences)\n",
    "def create_lstm_model(input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models[\"LSTM\"] = create_lstm_model(input_length=your_padded_sequence_length)\n",
    "\n",
    "# Define a simple GRU model (use with padded sequences)\n",
    "def create_gru_model(input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=input_length))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models[\"GRU\"] = create_gru_model(input_length=your_padded_sequence_length)\n",
    "\n",
    "# Define a DistilBERT model (use with encoded input from the Hugging Face tokenizer)\n",
    "def create_distilbert_model():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "    model.compile(optimizer='adam', loss=model.compute_loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models[\"DistilBERT\"] = create_distilbert_model()\n",
    "\n",
    "# You can also include other transformer-based models like BERT, GPT, RoBERTa, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(text):\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding='max_length', truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame with columns 'text' (article text) and 'label' (binary label for genuine/fake news)\n",
    "data['inputs'] = data['text'].apply(tokenize)\n",
    "\n",
    "def create_bert_model():\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    optimizer = Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "    model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models[\"BERT\"] = create_bert_model()\n",
    "\n",
    "X = np.stack(data['inputs'].values)\n",
    "y = data['label'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the model\n",
    "models[\"BERT\"].fit(X_train, y_train, batch_size=16, epochs=3, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, GPT2Config\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def tokenize(text):\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding='max_length', truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame with columns 'text' (article text) and 'label' (binary label for genuine/fake news)\n",
    "data['inputs'] = data['text'].apply(tokenize)\n",
    "\n",
    "def create_gpt2_model():\n",
    "    gpt2_config = GPT2Config.from_pretrained('gpt2', output_hidden_states=True)\n",
    "    gpt2_model = TFGPT2LMHeadModel.from_pretrained('gpt2', config=gpt2_config)\n",
    "\n",
    "    # Define the classification head\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32)\n",
    "    outputs = gpt2_model(input_ids, attention_mask=attention_mask)\n",
    "    hidden_states = outputs.hidden_states[-1][:, 0]\n",
    "    classification_output = Dense(2, activation='softmax')(hidden_states)\n",
    "\n",
    "    # Combine GPT-2 and the classification head\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=classification_output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models[\"GPT-2\"] = create_gpt2_model()\n",
    "X = np.stack(data['inputs'].values)\n",
    "y = data['label'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the model\n",
    "models[\"GPT-2\"].fit(X_train, y_train, batch_size=8, epochs=3, validation_split=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
